{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c63c6098-d97a-4dc1-b98a-941984c15ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from typing import List, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class DocumentChunk:\n",
    "    \"\"\"Represents a single document chunk with metadata\"\"\"\n",
    "    content: str\n",
    "    page_number: int\n",
    "    side: str  # 'LEFT' or 'RIGHT'\n",
    "    chunk_id: str\n",
    "    metadata: Dict = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.metadata is None:\n",
    "            self.metadata = {}\n",
    "        \n",
    "        # Add basic metadata\n",
    "        self.metadata.update({\n",
    "            'page_number': self.page_number,\n",
    "            'side': self.side,\n",
    "            'chunk_id': self.chunk_id,\n",
    "            'word_count': len(self.content.split()),\n",
    "            'char_count': len(self.content)\n",
    "        })\n",
    "\n",
    "class DocumentPageChunker:\n",
    "    \"\"\"\n",
    "    Chunks documents based on page markers for RAG applications.\n",
    "    Handles both LEFT and RIGHT page sections.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Regex pattern to match page markers\n",
    "        self.page_pattern = r'--- Page (\\d+) \\((LEFT|MIDDLE|RIGHT)\\) ---'\n",
    "    \n",
    "    def chunk_document(self, text: str, \n",
    "                      min_chunk_length: int = 50,\n",
    "                      include_empty_chunks: bool = False) -> List[DocumentChunk]:\n",
    "        \"\"\"\n",
    "        Chunk document by page markers.\n",
    "        \n",
    "        Args:\n",
    "            text: The document text to chunk\n",
    "            min_chunk_length: Minimum character length for a chunk to be included\n",
    "            include_empty_chunks: Whether to include chunks with no content\n",
    "            \n",
    "        Returns:\n",
    "            List of DocumentChunk objects\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        # Find all page markers with their positions\n",
    "        matches = list(re.finditer(self.page_pattern, text))\n",
    "        \n",
    "        if not matches:\n",
    "            # If no page markers found, return the entire text as a single chunk\n",
    "            return [DocumentChunk(\n",
    "                content=text.strip(),\n",
    "                page_number=1,\n",
    "                side='UNKNOWN',\n",
    "                chunk_id='chunk_1_UNKNOWN'\n",
    "            )]\n",
    "        \n",
    "        # Process each page section\n",
    "        for i, match in enumerate(matches):\n",
    "            page_num = int(match.group(1))\n",
    "            side = match.group(2)\n",
    "            \n",
    "            # Determine start and end positions for this chunk\n",
    "            start_pos = match.end()  # Start after the page marker\n",
    "            \n",
    "            if i + 1 < len(matches):\n",
    "                # End before the next page marker\n",
    "                end_pos = matches[i + 1].start()\n",
    "            else:\n",
    "                # Last chunk goes to end of document\n",
    "                end_pos = len(text)\n",
    "            \n",
    "            # Extract and clean the content\n",
    "            content = text[start_pos:end_pos].strip()\n",
    "            \n",
    "            # Apply filtering based on parameters\n",
    "            if not include_empty_chunks and len(content) == 0:\n",
    "                continue\n",
    "                \n",
    "            if len(content) < min_chunk_length:\n",
    "                continue\n",
    "            \n",
    "            # Create chunk ID\n",
    "            chunk_id = f\"chunk_{page_num}_{side}\"\n",
    "            \n",
    "            # Create the chunk object\n",
    "            chunk = DocumentChunk(\n",
    "                content=content,\n",
    "                page_number=page_num,\n",
    "                side=side,\n",
    "                chunk_id=chunk_id\n",
    "            )\n",
    "            \n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def chunk_from_file(self, file_path: str, **kwargs) -> List[DocumentChunk]:\n",
    "        \"\"\"\n",
    "        Chunk document directly from a file.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the text file\n",
    "            **kwargs: Additional arguments passed to chunk_document\n",
    "            \n",
    "        Returns:\n",
    "            List of DocumentChunk objects\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "            return self.chunk_document(text, **kwargs)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error reading file {file_path}: {str(e)}\")\n",
    "    \n",
    "    def get_chunks_summary(self, chunks: List[DocumentChunk]) -> Dict:\n",
    "        \"\"\"\n",
    "        Get summary statistics about the chunks.\n",
    "        \n",
    "        Args:\n",
    "            chunks: List of DocumentChunk objects\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with summary statistics\n",
    "        \"\"\"\n",
    "        if not chunks:\n",
    "            return {'total_chunks': 0}\n",
    "        \n",
    "        pages = set(chunk.page_number for chunk in chunks)\n",
    "        sides = set(chunk.side for chunk in chunks)\n",
    "        word_counts = [chunk.metadata['word_count'] for chunk in chunks]\n",
    "        char_counts = [chunk.metadata['char_count'] for chunk in chunks]\n",
    "        \n",
    "        return {\n",
    "            'total_chunks': len(chunks),\n",
    "            'unique_pages': len(pages),\n",
    "            'page_range': f\"{min(pages)}-{max(pages)}\" if pages else \"N/A\",\n",
    "            'sides_present': list(sides),\n",
    "            'avg_word_count': sum(word_counts) / len(word_counts),\n",
    "            'avg_char_count': sum(char_counts) / len(char_counts),\n",
    "            'min_word_count': min(word_counts),\n",
    "            'max_word_count': max(word_counts),\n",
    "            'total_words': sum(word_counts)\n",
    "        }\n",
    "    \n",
    "    def export_chunks_for_rag(self, chunks: List[DocumentChunk], \n",
    "                             format_type: str = 'dict') -> List:\n",
    "        \"\"\"\n",
    "        Export chunks in a format suitable for RAG systems.\n",
    "        \n",
    "        Args:\n",
    "            chunks: List of DocumentChunk objects\n",
    "            format_type: Export format ('dict', 'text', or 'structured')\n",
    "            \n",
    "        Returns:\n",
    "            List of chunks in the specified format\n",
    "        \"\"\"\n",
    "        if format_type == 'dict':\n",
    "            return [\n",
    "                {\n",
    "                    'id': chunk.chunk_id,\n",
    "                    'content': chunk.content,\n",
    "                    'metadata': chunk.metadata\n",
    "                }\n",
    "                for chunk in chunks\n",
    "            ]\n",
    "        \n",
    "        elif format_type == 'text':\n",
    "            return [chunk.content for chunk in chunks]\n",
    "        \n",
    "        elif format_type == 'structured':\n",
    "            return [\n",
    "                {\n",
    "                    'id': chunk.chunk_id,\n",
    "                    'text': chunk.content,\n",
    "                    'page': chunk.page_number,\n",
    "                    'side': chunk.side,\n",
    "                    'word_count': chunk.metadata['word_count'],\n",
    "                    'source': f\"Page {chunk.page_number} ({chunk.side})\"\n",
    "                }\n",
    "                for chunk in chunks\n",
    "            ]\n",
    "        \n",
    "    def save_chunks(self, chunks: List[DocumentChunk], \n",
    "                    output_dir: str = \"chunks_output\",\n",
    "                    formats: List[str] = ['json', 'jsonl', 'csv', 'txt']) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Save chunks to files in multiple formats for RAG systems.\n",
    "        \n",
    "        Args:\n",
    "            chunks: List of DocumentChunk objects\n",
    "            output_dir: Directory to save files\n",
    "            formats: List of formats to save ('json', 'jsonl', 'csv', 'txt', 'markdown')\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with format -> file_path mappings\n",
    "        \"\"\"\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        saved_files = {}\n",
    "        \n",
    "        # Prepare data for export\n",
    "        chunk_data = self.export_chunks_for_rag(chunks, format_type='structured')\n",
    "        \n",
    "        # Save as JSON (single file with all chunks)\n",
    "        if 'json' in formats:\n",
    "            json_path = os.path.join(output_dir, 'chunks.json')\n",
    "            with open(json_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(chunk_data, f, indent=2, ensure_ascii=False)\n",
    "            saved_files['json'] = json_path\n",
    "        \n",
    "        # Save as JSONL (one JSON object per line - great for streaming)\n",
    "        if 'jsonl' in formats:\n",
    "            jsonl_path = os.path.join(output_dir, 'chunks.jsonl')\n",
    "            with open(jsonl_path, 'w', encoding='utf-8') as f:\n",
    "                for chunk in chunk_data:\n",
    "                    f.write(json.dumps(chunk, ensure_ascii=False) + '\\n')\n",
    "            saved_files['jsonl'] = jsonl_path\n",
    "        \n",
    "        # Save as CSV (easy to import into databases)\n",
    "        if 'csv' in formats:\n",
    "            csv_path = os.path.join(output_dir, 'chunks.csv')\n",
    "            with open(csv_path, 'w', newline='', encoding='utf-8') as f:\n",
    "                if chunk_data:\n",
    "                    fieldnames = chunk_data[0].keys()\n",
    "                    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "                    writer.writeheader()\n",
    "                    for chunk in chunk_data:\n",
    "                        writer.writerow(chunk)\n",
    "            saved_files['csv'] = csv_path\n",
    "        \n",
    "        # Save as separate text files (useful for vector databases)\n",
    "        if 'txt' in formats:\n",
    "            txt_dir = os.path.join(output_dir, 'txt_files')\n",
    "            os.makedirs(txt_dir, exist_ok=True)\n",
    "            for chunk in chunk_data:\n",
    "                filename = f\"{chunk['id']}.txt\"\n",
    "                txt_path = os.path.join(txt_dir, filename)\n",
    "                with open(txt_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(f\"Source: {chunk['source']}\\n\")\n",
    "                    f.write(f\"Page: {chunk['page']}, Side: {chunk['side']}\\n\")\n",
    "                    f.write(f\"Word Count: {chunk['word_count']}\\n\")\n",
    "                    f.write(\"-\" * 50 + \"\\n\")\n",
    "                    f.write(chunk['text'])\n",
    "            saved_files['txt'] = txt_dir\n",
    "        \n",
    "        # Save as Markdown (great for documentation and readability)\n",
    "        if 'markdown' in formats:\n",
    "            md_path = os.path.join(output_dir, 'chunks.md')\n",
    "            with open(md_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(\"# Document Chunks\\n\\n\")\n",
    "                for chunk in chunk_data:\n",
    "                    f.write(f\"## {chunk['id']}\\n\\n\")\n",
    "                    f.write(f\"**Source:** {chunk['source']}  \\n\")\n",
    "                    f.write(f\"**Word Count:** {chunk['word_count']}  \\n\\n\")\n",
    "                    f.write(chunk['text'])\n",
    "                    f.write(\"\\n\\n---\\n\\n\")\n",
    "            saved_files['markdown'] = md_path\n",
    "        \n",
    "        return saved_files\n",
    "\n",
    "# Convenience functions for easy importing and usage\n",
    "def chunk_file(file_path: str, min_chunk_length: int = 50, \n",
    "               include_empty_chunks: bool = False) -> List[DocumentChunk]:\n",
    "    \"\"\"\n",
    "    Simple function to chunk a file - easy to import and use.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to your text file\n",
    "        min_chunk_length: Minimum character length for chunks\n",
    "        include_empty_chunks: Whether to include empty sections\n",
    "        \n",
    "    Returns:\n",
    "        List of DocumentChunk objects\n",
    "    \"\"\"\n",
    "    chunker = DocumentPageChunker()\n",
    "    return chunker.chunk_from_file(file_path, \n",
    "                                  min_chunk_length=min_chunk_length,\n",
    "                                  include_empty_chunks=include_empty_chunks)\n",
    "\n",
    "def chunk_text(text: str, min_chunk_length: int = 50, \n",
    "               include_empty_chunks: bool = False) -> List[DocumentChunk]:\n",
    "    \"\"\"\n",
    "    Simple function to chunk text - easy to import and use.\n",
    "    \n",
    "    Args:\n",
    "        text: Document text to chunk\n",
    "        min_chunk_length: Minimum character length for chunks  \n",
    "        include_empty_chunks: Whether to include empty sections\n",
    "        \n",
    "    Returns:\n",
    "        List of DocumentChunk objects\n",
    "    \"\"\"\n",
    "    chunker = DocumentPageChunker()\n",
    "    return chunker.chunk_document(text, \n",
    "                                 min_chunk_length=min_chunk_length,\n",
    "                                 include_empty_chunks=include_empty_chunks)\n",
    "\n",
    "def get_rag_chunks(file_path: str, format_type: str = 'structured') -> List[Dict]:\n",
    "    \"\"\"\n",
    "    One-liner to get RAG-ready chunks from a file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to your text file\n",
    "        format_type: 'dict', 'text', or 'structured'\n",
    "        \n",
    "    Returns:\n",
    "        List of chunks ready for RAG system\n",
    "    \"\"\"\n",
    "    chunks = chunk_file(file_path)\n",
    "    chunker = DocumentPageChunker()\n",
    "    return chunker.export_chunks_for_rag(chunks, format_type=format_type)\n",
    "\n",
    "def save_chunks_to_files(file_path: str, output_dir: str = \"chunks_output\", \n",
    "                        formats: List[str] = ['json', 'jsonl', 'csv']) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    One-liner to chunk a file and save in multiple formats for RAG.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to your text file\n",
    "        output_dir: Directory to save chunks\n",
    "        formats: List of formats ('json', 'jsonl', 'csv', 'txt', 'markdown')\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with format -> file_path mappings\n",
    "    \"\"\"\n",
    "    chunker = DocumentPageChunker()\n",
    "    chunks = chunker.chunk_from_file(file_path)\n",
    "    return chunker.save_chunks(chunks, output_dir=output_dir, formats=formats)\n",
    "\n",
    "def create_rag_dataset(file_path: str, output_dir: str = \"rag_dataset\") -> str:\n",
    "    \"\"\"\n",
    "    Create a complete RAG dataset from your document.\n",
    "    Saves in multiple formats optimized for different RAG systems.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to your text file\n",
    "        output_dir: Directory to save the dataset\n",
    "        \n",
    "    Returns:\n",
    "        Path to the created dataset directory\n",
    "    \"\"\"\n",
    "    print(f\"Creating RAG dataset from: {file_path}\")\n",
    "    \n",
    "    # Chunk the document\n",
    "    chunker = DocumentPageChunker()\n",
    "    chunks = chunker.chunk_from_file(file_path)\n",
    "    \n",
    "    print(f\"Found {len(chunks)} chunks\")\n",
    "    \n",
    "    # Save in all formats\n",
    "    saved_files = chunker.save_chunks(\n",
    "        chunks, \n",
    "        output_dir=output_dir,\n",
    "        formats=['json', 'jsonl', 'csv', 'txt', 'markdown']\n",
    "    )\n",
    "    \n",
    "    # Create a README file\n",
    "    readme_path = os.path.join(output_dir, 'README.md')\n",
    "    with open(readme_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"# RAG Dataset\\n\\n\")\n",
    "        f.write(f\"Generated from: `{file_path}`\\n\\n\")\n",
    "        f.write(f\"Total chunks: {len(chunks)}\\n\\n\")\n",
    "        \n",
    "        # Get summary statistics\n",
    "        summary = chunker.get_chunks_summary(chunks)\n",
    "        f.write(\"## Dataset Statistics\\n\\n\")\n",
    "        for key, value in summary.items():\n",
    "            f.write(f\"- **{key.replace('_', ' ').title()}**: {value}\\n\")\n",
    "        \n",
    "        f.write(\"\\n## Available Formats\\n\\n\")\n",
    "        for format_name, file_path in saved_files.items():\n",
    "            f.write(f\"- **{format_name.upper()}**: `{file_path}`\\n\")\n",
    "        \n",
    "        f.write(\"\\n## Usage Examples\\n\\n\")\n",
    "        f.write(\"### Load JSON chunks\\n\")\n",
    "        f.write(\"```python\\n\")\n",
    "        f.write(\"import json\\n\")\n",
    "        f.write(f\"with open('{saved_files['json']}', 'r') as f:\\n\")\n",
    "        f.write(\"    chunks = json.load(f)\\n\")\n",
    "        f.write(\"```\\n\\n\")\n",
    "        \n",
    "        f.write(\"### Load JSONL chunks (streaming)\\n\")\n",
    "        f.write(\"```python\\n\")\n",
    "        f.write(\"import json\\n\")\n",
    "        f.write(\"chunks = []\\n\")\n",
    "        f.write(f\"with open('{saved_files['jsonl']}', 'r') as f:\\n\")\n",
    "        f.write(\"    for line in f:\\n\")\n",
    "        f.write(\"        chunks.append(json.loads(line))\\n\")\n",
    "        f.write(\"```\\n\")\n",
    "    \n",
    "    print(f\"\\nRAG dataset created successfully!\")\n",
    "    print(f\"Location: {output_dir}\")\n",
    "    print(\"\\nFiles created:\")\n",
    "    for format_name, file_path in saved_files.items():\n",
    "        print(f\"  {format_name.upper()}: {file_path}\")\n",
    "    print(f\"  README: {readme_path}\")\n",
    "    \n",
    "    return output_dir\n",
    "\n",
    "# # Example usage\n",
    "# def example_usage():\n",
    "#     \"\"\"Shows how to use the chunker with your file and save results\"\"\"\n",
    "    \n",
    "#     # Replace with your actual file path\n",
    "#     file_path = 'paste.txt'\n",
    "    \n",
    "#     try:\n",
    "#         # Method 1: Simple save in multiple formats\n",
    "#         print(\"=== METHOD 1: Save chunks in multiple formats ===\")\n",
    "#         saved_files = save_chunks_to_files(\n",
    "#             file_path, \n",
    "#             output_dir=\"my_chunks\",\n",
    "#             formats=['json', 'jsonl', 'csv']\n",
    "#         )\n",
    "#         print(\"Saved files:\", saved_files)\n",
    "        \n",
    "#         # Method 2: Create complete RAG dataset\n",
    "#         print(\"\\n=== METHOD 2: Create complete RAG dataset ===\")\n",
    "#         dataset_dir = create_rag_dataset(file_path, output_dir=\"tbi_rag_dataset\")\n",
    "        \n",
    "#         # Method 3: Load and use the chunks\n",
    "#         print(\"\\n=== METHOD 3: Load saved chunks ===\")\n",
    "#         with open(os.path.join(dataset_dir, 'chunks.json'), 'r') as f:\n",
    "#             loaded_chunks = json.load(f)\n",
    "        \n",
    "#         print(f\"Loaded {len(loaded_chunks)} chunks\")\n",
    "#         if loaded_chunks:\n",
    "#             print(\"First chunk example:\")\n",
    "#             first_chunk = loaded_chunks[0]\n",
    "#             print(f\"  ID: {first_chunk['id']}\")\n",
    "#             print(f\"  Source: {first_chunk['source']}\")\n",
    "#             print(f\"  Text preview: {first_chunk['text'][:100]}...\")\n",
    "            \n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"File not found: {file_path}\")\n",
    "#         print(\"Make sure your file path is correct!\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error: {e}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     example_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cdb6d83-34ba-42cd-abc4-daef4bb0415d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating RAG dataset from: scn_output/scn_text.txt\n",
      "Found 23 chunks\n",
      "\n",
      "RAG dataset created successfully!\n",
      "Location: scn_output/chunks/\n",
      "\n",
      "Files created:\n",
      "  JSON: scn_output/chunks/chunks.json\n",
      "  JSONL: scn_output/chunks/chunks.jsonl\n",
      "  CSV: scn_output/chunks/chunks.csv\n",
      "  TXT: scn_output/chunks/txt_files\n",
      "  MARKDOWN: scn_output/chunks/chunks.md\n",
      "  README: scn_output/chunks/README.md\n"
     ]
    }
   ],
   "source": [
    "# # Save this code as 'document_chunker.py'\n",
    "# from document_chunker import create_rag_dataset, save_chunks_to_files, chunk_file\n",
    "article='scn'\n",
    "# Method 1: Create complete RAG dataset (recommended)\n",
    "dataset_dir = create_rag_dataset(f'{article}_output/{article}_text.txt', output_dir=f'{article}_output/chunks/')\n",
    "\n",
    "# # Method 2: Save in specific formats\n",
    "# saved_files = save_chunks_to_files(\n",
    "#     'paste.txt', \n",
    "#     output_dir='chunks_output',\n",
    "#     formats=['json', 'jsonl', 'csv']\n",
    "# )\n",
    "\n",
    "# # Method 3: Just get chunks without saving\n",
    "# chunks = chunk_file('paste.txt')\n",
    "# print(f\"Found {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bf3e5a-5330-487d-8479-66128e47329a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fb3138-6d6d-4b61-96e6-c5b8afdac743",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_env",
   "language": "python",
   "name": "cv_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
